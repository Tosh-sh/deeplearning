{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "epoch: 1\n",
      "loss: 3.522, val_loss: 3.48\n",
      "> at the end of a working day everybody is in a hurry to get home .\n",
      "= 一 日 の 仕事 が 終わ る と 皆 家路 を 急 ぐ 。 </s>\n",
      "< 彼 は は は は は は は は は は は は は を を い い 。 </s>\n",
      "\n",
      "> i am as much in the wrong as you are about not writing before this .\n",
      "= ご 無 沙汰 は お 互い さま で す 。 </s>\n",
      "< 彼 は は は は は は は は は は は は は は を を を い 。\n",
      "\n",
      "> my father boasts of the fact that he has never had a traffic accident .\n",
      "= 父 は 車 の 無 事故 を 自慢 し て い る 。 </s>\n",
      "< 彼 は は は は は は は は は は は は は は を を を い 。\n",
      "\n",
      "> mr smith and i have been acquainted with each other for a long time .\n",
      "= スミス 私 と は 長 い 間 の 知り合い で す 。 </s>\n",
      "< 彼 は は は は は は は は は は は は は は を を い い 。\n",
      "\n",
      "> it is hard for an old man to change his way of thinking .\n",
      "= 老人 が 考え を 変え る の は 難し い 。 </s>\n",
      "< 彼 は は は は は は は は は は は は を を い い 。 </s> 。\n",
      "\n",
      "> as soon as you get the wall painted , you can go home .\n",
      "= 壁 を <unk> 終え たら 、 すぐ に 帰宅 し て も い い よ 。 </s>\n",
      "< 彼 は は は は は は は は は は は を を を い 。 </s> 。 </s>\n",
      "\n",
      "> i haven 't made up my mind <unk> enough to agree with you .\n",
      "= まだ 決心 が つ か な い の で 同意 でき ま せ ん 。 </s>\n",
      "< 彼 は は は は は は は は は は は は を を い い 。 </s> 。\n",
      "\n",
      "> if you are going to have a party , please count me in .\n",
      "= もし パーティー を 開 く なら 私 も 仲間 に 入れ て 下さ い 。 </s>\n",
      "< 彼 は は は は は は は は は は は を を い い 。 </s> 。 </s>\n",
      "\n",
      "> when he got into trouble , he turned to his parents for help .\n",
      "= 困難 に 、 彼 は 両親 に 助け を 求め た 。 </s>\n",
      "< 彼 は は は は は は は は は は は は を を を い 。 </s> 。\n",
      "\n",
      "> i sometimes look back on the good days i had in london .\n",
      "= ロンドン で 過ご し た 楽し い 日々 の こと を 時々 思い出 す 。 </s>\n",
      "< 彼 は は は は は は は は は は は を を を い 。 </s> 。 </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Encoder-Decoder - TensorFlow\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import metrics\n",
    "from utils import Vocab\n",
    "from utils.tf import DataLoader\n",
    "\n",
    "\n",
    "class EncoderDecoder(Model):\n",
    "    def __init__(self,\n",
    "                 input_dim,\n",
    "                 hidden_dim,\n",
    "                 output_dim,\n",
    "                 maxlen=20):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(input_dim, hidden_dim)\n",
    "        self.decoder = Decoder(hidden_dim, output_dim)\n",
    "\n",
    "        self.maxlen = maxlen\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def call(self, source, target=None, use_teacher_forcing=False):\n",
    "        batch_size = source.shape[0]\n",
    "        if target is not None:\n",
    "            len_target_sequences = target.shape[1]\n",
    "        else:\n",
    "            len_target_sequences = self.maxlen\n",
    "\n",
    "        _, states = self.encoder(source)\n",
    "\n",
    "        y = tf.ones((batch_size, 1), dtype=tf.int32)\n",
    "        output = tf.zeros((batch_size, 1, self.output_dim), dtype=tf.float32)\n",
    "\n",
    "        for t in range(len_target_sequences):\n",
    "            out, states = self.decoder(y, states)\n",
    "            out = out[:, tf.newaxis]\n",
    "            output = tf.concat([output, out], axis=1)\n",
    "\n",
    "            if use_teacher_forcing and target is not None:\n",
    "                y = target[:, t][:, tf.newaxis]\n",
    "            else:\n",
    "                y = tf.argmax(out, axis=-1, output_type=tf.int32)\n",
    "\n",
    "        return output[:, 1:]\n",
    "\n",
    "\n",
    "class Encoder(Model):\n",
    "    def __init__(self,\n",
    "                 input_dim,\n",
    "                 hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = Embedding(input_dim, hidden_dim, mask_zero=True)\n",
    "        self.lstm = LSTM(hidden_dim, activation='tanh',\n",
    "                         recurrent_activation='sigmoid',\n",
    "                         kernel_initializer='glorot_normal',\n",
    "                         recurrent_initializer='orthogonal',\n",
    "                         return_state=True)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.embedding(x)\n",
    "        h, state_h, state_c = self.lstm(x)\n",
    "\n",
    "        return h, (state_h, state_c)\n",
    "\n",
    "\n",
    "class Decoder(Model):\n",
    "    def __init__(self,\n",
    "                 hidden_dim,\n",
    "                 output_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = Embedding(output_dim, hidden_dim)\n",
    "        self.lstm = LSTM(hidden_dim, activation='tanh',\n",
    "                         recurrent_activation='sigmoid',\n",
    "                         kernel_initializer='glorot_normal',\n",
    "                         recurrent_initializer='orthogonal',\n",
    "                         return_state=True)\n",
    "        self.out = Dense(output_dim, kernel_initializer='glorot_normal',\n",
    "                         activation='softmax')\n",
    "\n",
    "    def call(self, x, states):\n",
    "        x = self.embedding(x)\n",
    "        h, state_h, state_c = self.lstm(x, states)\n",
    "        y = self.out(h)\n",
    "\n",
    "        return y, (state_h, state_c)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    np.random.seed(123)\n",
    "    tf.random.set_seed(123)\n",
    "\n",
    "    '''\n",
    "    1. データの準備\n",
    "    '''\n",
    "    data_dir = os.path.join(os.path.dirname('__file__'), 'data')\n",
    "\n",
    "    en_train_path = os.path.join(data_dir, 'train.en')\n",
    "    en_val_path = os.path.join(data_dir, 'dev.en')\n",
    "    en_test_path = os.path.join(data_dir, 'test.en')\n",
    "\n",
    "    ja_train_path = os.path.join(data_dir, 'train.ja')\n",
    "    ja_val_path = os.path.join(data_dir, 'dev.ja')\n",
    "    ja_test_path = os.path.join(data_dir, 'test.ja')\n",
    "\n",
    "    en_vocab = Vocab()\n",
    "    ja_vocab = Vocab()\n",
    "\n",
    "    en_vocab.fit(en_train_path)\n",
    "    ja_vocab.fit(ja_train_path)\n",
    "\n",
    "    x_train = en_vocab.transform(en_train_path)\n",
    "    x_val = en_vocab.transform(en_val_path)\n",
    "    x_test = en_vocab.transform(en_test_path)\n",
    "\n",
    "    t_train = ja_vocab.transform(ja_train_path, eos=True)\n",
    "    t_val = ja_vocab.transform(ja_val_path, eos=True)\n",
    "    t_test = ja_vocab.transform(ja_test_path, eos=True)\n",
    "\n",
    "    def sort(x, t):\n",
    "        lens = [len(i) for i in x]\n",
    "        indices = sorted(range(len(lens)), key=lambda i: -lens[i])\n",
    "        x = [x[i] for i in indices]\n",
    "        t = [t[i] for i in indices]\n",
    "\n",
    "        return (x, t)\n",
    "\n",
    "    (x_train, t_train) = sort(x_train, t_train)\n",
    "    (x_val, t_val) = sort(x_val, t_val)\n",
    "    (x_test, t_test) = sort(x_test, t_test)\n",
    "\n",
    "    train_dataloader = DataLoader((x_train, t_train))\n",
    "    val_dataloader = DataLoader((x_val, t_val))\n",
    "    test_dataloader = DataLoader((x_test, t_test), batch_size=1)\n",
    "\n",
    "    '''\n",
    "    2. モデルの構築\n",
    "    '''\n",
    "    depth_x = len(en_vocab.i2w)\n",
    "    depth_t = len(ja_vocab.i2w)\n",
    "\n",
    "    input_dim = depth_x\n",
    "    hidden_dim = 128\n",
    "    output_dim = depth_t\n",
    "\n",
    "    model = EncoderDecoder(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "    '''\n",
    "    3. モデルの学習・評価\n",
    "    '''\n",
    "    criterion = tf.losses.CategoricalCrossentropy()\n",
    "    optimizer = optimizers.Adam(learning_rate=0.001,\n",
    "                                beta_1=0.9, beta_2=0.999, amsgrad=True)\n",
    "    train_loss = metrics.Mean()\n",
    "    val_loss = metrics.Mean()\n",
    "\n",
    "    def compute_loss(t, y):\n",
    "        return criterion(t, y)\n",
    "\n",
    "    def train_step(x, t, depth_t,\n",
    "                   teacher_forcing_rate=0.5):\n",
    "        use_teacher_forcing = (random.random() < teacher_forcing_rate)\n",
    "        with tf.GradientTape() as tape:\n",
    "            preds = model(x, t, use_teacher_forcing=use_teacher_forcing)\n",
    "            mask_t = tf.cast(tf.not_equal(t, 0), tf.float32)\n",
    "            t = tf.one_hot(t, depth=depth_t, dtype=tf.float32)\n",
    "            t = t * mask_t[:, :, tf.newaxis]\n",
    "            loss = compute_loss(t, preds)\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        train_loss(loss)\n",
    "\n",
    "        return preds\n",
    "\n",
    "    def val_step(x, t, depth_t):\n",
    "        preds = model(x, t, use_teacher_forcing=False)\n",
    "        mask_t = tf.cast(tf.not_equal(t, 0), tf.float32)\n",
    "        t = tf.one_hot(t, depth=depth_t, dtype=tf.float32)\n",
    "        t = t * mask_t[:, :, tf.newaxis]\n",
    "        loss = compute_loss(t, preds)\n",
    "        val_loss(loss)\n",
    "\n",
    "        return preds\n",
    "\n",
    "    def test_step(x):\n",
    "        preds = model(x)\n",
    "        return preds\n",
    "\n",
    "    epochs = 1\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print('-' * 20)\n",
    "        print('epoch: {}'.format(epoch+1))\n",
    "\n",
    "        for (x, t) in train_dataloader:\n",
    "            train_step(x, t, depth_t)\n",
    "\n",
    "        for (x, t) in val_dataloader:\n",
    "            val_step(x, t, depth_t)\n",
    "\n",
    "        print('loss: {:.3f}, val_loss: {:.3}'.format(\n",
    "            train_loss.result(),\n",
    "            val_loss.result()\n",
    "        ))\n",
    "\n",
    "        for idx, (x, t) in enumerate(test_dataloader):\n",
    "            preds = test_step(x)\n",
    "\n",
    "            source = x.numpy().reshape(-1)\n",
    "            target = t.numpy().reshape(-1)\n",
    "            out = tf.argmax(preds, axis=-1).numpy().reshape(-1)\n",
    "\n",
    "            source = ' '.join(en_vocab.decode(source))\n",
    "            target = ' '.join(ja_vocab.decode(target))\n",
    "            out = ' '.join(ja_vocab.decode(out))\n",
    "\n",
    "            print('>', source)\n",
    "            print('=', target)\n",
    "            print('<', out)\n",
    "            print()\n",
    "\n",
    "            if idx >= 9:\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('honyaku.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
